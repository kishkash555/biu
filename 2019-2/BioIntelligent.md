<div dir='rtl'>

### התרגיל1 
תרגיל: לממש רשת נוירונים בנמפיי בלבד. 

כל ספריה של למידת מכונה אסורה.

forward pass, training, back propagation.

מי שכותב בC, C++ יקבל בונוס.

מגישים פרדיקציות על טסט דאטה.

כל שורה מתחילה בלייבל. יש 10 קטגוריות. 3000 פיצ'רים. תחליטו כמה שכבות hidden.
פלט 10 נוירונים. 
להגיש: קוד, הוראות, הרצה.

תקבעו את ה seed ככה שתמיד יצא אותו דבר.

###
overfitting היא עליית ציון ב trainining שלא מותאמת ב test.

מה אפשר לעשות כדי "להפריע" לרשת ללמוד "בע"פ". פעם אמרו תוריד את מספר הפרמטרים, היום מבינים שזה לא נכון, אתה עלול לפגוע ביכולת הכללה.

איך מפריעים? במישור הקלט או במישור הלמידה Data augmentation

#### Data augmentation methods
פעם חשבו שאם הדאטה ענק לא יהיה OF. זה לא נכון. 
* input noise בהסתברות מסויימת נוסיף רעש, למשל גאוסיאני לפיקסלים. היום במקום גאוסיאן מכבים למשל 20% מהפיקסלים. תעשו את זה זה עוזר במיידי.
יש sweetspot איפשהו בין 0% רעש ל 100% רעש.
בכל epoch פיקסלים אחרים יוצאים.

דיון בכיתה - למה דווקא לאפס את הפיקסל? אולי עדיף להחליף לממוצע? 
* הזזות והיפוכים- לשים לב כי זה domain specific. בדומה jittering, elastic.

#### avoiding overfitting during training
הפרעה אחרי למידה היא לא אפקטיבית כי אתה לא יודע איפה להתערב.
* weight decay - איתרתי את המאמר המקורי משנות השמונים. השראה מהמוח האנושי. 
כוח הרסני על כל הסינפסות. כשאנחנו ישנים הסינפסות דועכות. משתמשים בנורמה הראשונה או השנייה. 
היה לנו
</div>

$w'=w+\alpha\cdot x \cdot \Delta$

עכשיו נוסיף פרמטר

$w'=w+\alpha\cdot x \cdot \Delta- d\cdot w,\ d \approx 1/10,000$

<div dir='rtl'>

עד האפוק הבא,שבו הם יראו את החתול שוב. אם הסינפסה חשובה, התוספת תהיה בטלה. אם באמת לא משתמשים בזה, עד אותו חתול שוב זה יעלם.
&alpha; יורד עם הזמן לכן חשוב להכניס את הd:

</div>

$w'=w+\alpha\cdot (x \cdot \Delta- d\cdot w)$


<div dir='rtl'>

זה שהW מופיע פה לינארית, זה אומר שעשינו עונש פרופורציוני לנגזרת שניה.
אם רוצים להעניש את הנגזרת הראשונה

</div>

$w'=w+\alpha\cdot (x \cdot \Delta- \min(d,abs(w))\cdot sign(w) \cdot w)$


<div dir='rtl'>
כשהתחילו על ReLu ישר שמו רגולריזציה.
למה עדיף L2? כי L1 מעניש את כולם באותה מידה. L2 יותר הגיוני. אתה לא רוצה להפריע למשקולות קטנים, הם מייצגים מידע.

### DropOut
עבור כל בעיה, נאמן רשת מס. 1, נאמן שוב נקבל רשת מס' 2 . ככה כמה פעמים. אם נעשה ממוצע של כולם נקבל דיוק יותר טוב. אבל לא עושים את זה כי זה יוצר עומס חישוב. איך אפשר להרוויח את אותו אפקט בלי לשלם חישובית.? 
עבור כל דוגמה שמכניסים לרשת, באופן רנדומלי מכבים 50% מהנוירונים בשכבות הhidden. 

איך תממשו? תבנית של 0 ו1 אקראית ותכפילו את האקטיבציה. יש כאילו דגימה מהרבה רשתות שונות. במאמר כתוב prevents co-adaptation of neurons. 
כמו סטודנט שמציץ מעבר לכתב של הסטודנט הטוב. 
הופך את הנוירונים לעצמאיים יותר. 

רגע, בטסט הנוירונים יהיו חיים. מה הבעיה עם זה? יש הרבה יותר נוירונים שיורים לתוכו. מה עושים? אם הdropout rate היה do, אנחנו נכפיל ב$(do-1)$ יש כמה שיטות: סקאלה לפלט, סקאלה לפלט, סקאלה למשקולות. כל השיטות שקולות

### DropConnect
אותו רעיון רק על weights. הרבה יותר כבד חישובית והתוצאה פחות טובה.

