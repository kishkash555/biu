{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "STUDENT={'name': 'Shahar Siegman',\n",
    "         'ID': '011862141'}\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax vector.\n",
    "    x: a n-dim vector (numpy array)\n",
    "    returns: an n-dim vector (numpy array) of softmax values\n",
    "    \"\"\"\n",
    "    x0 = x-x.mean()\n",
    "    ret = np.exp(x0)\n",
    "    return ret / ret.sum()\n",
    "    # YOUR CODE HERE\n",
    "    # Your code should be fast, so use a vectorized implementation using numpy,\n",
    "    # don't use any loops.\n",
    "    # With a vectorized implementation, the code should be no more than 2 lines.\n",
    "    #\n",
    "    # For numeric stability, use the identify you proved in Ex 2 Q1.\n",
    "    # return x\n",
    "    ### why does the original code say return x?\n",
    "\n",
    "def classifier_output(x, params):\n",
    "    \"\"\"\n",
    "    Return the output layer (class probabilities) \n",
    "    of a log-linear classifier with given params on input x.\n",
    "    \"\"\"\n",
    "    W,b = params\n",
    "    # we use Z = xW + b, where x and b are row vectors\n",
    "    f_at_x = np.dot(x,W) + b\n",
    "    probs = softmax(f_at_x)\n",
    "    return probs\n",
    "\n",
    "def predict(x, params):\n",
    "    \"\"\"\n",
    "    Returnss the prediction (highest scoring class id) of a\n",
    "    a log-linear classifier with given parameters on input x.\n",
    "\n",
    "    params: a list of the form [(W, b)]\n",
    "    W: matrix\n",
    "    b: vector\n",
    "    \"\"\"\n",
    "    return np.argmax(classifier_output(x, params))\n",
    "\n",
    "def loss_and_gradients(x, y, params):\n",
    "    \"\"\"\n",
    "    Compute the loss and the gradients at point x with given parameters.\n",
    "    y is a scalar indicating the correct label.\n",
    "\n",
    "    returns:\n",
    "        loss,[gW,gb]\n",
    "\n",
    "    loss: scalar\n",
    "    gW: matrix, gradients of W\n",
    "    gb: vector, gradients of b\n",
    "    \"\"\"\n",
    "    W,b = params\n",
    "    # todo: check the shape of b\n",
    "    shape_x = x.shape\n",
    "    if len(shape_x)>1 and shape_x[1] != 1:\n",
    "        print(\"x should be a column vector, actual shape: {}\".format(shape_x))\n",
    "        raise AssertionError\n",
    "    in_dim = shape_x[0]\n",
    "    shape_W = W.shape\n",
    "    if shape_W[0] != in_dim:\n",
    "        print(\"number of rows of W ({}) mismatches length of X ({})\".format(shape_W[0],in_dim))\n",
    "        raise AssertionError\n",
    "    out_dim = shape_W[1]\n",
    "    shape_b = b.shape\n",
    "    if len(shape_b)>1 and shape_b[1] != 1:\n",
    "        print(\"b is not a row vector, actual shape: {}\".format(shape_b))\n",
    "        raise AssertionError\n",
    "    if shape_b[0] != out_dim:\n",
    "        print(\"length of b ({}) mismatches columns of W ({})\".format(shape_b[1],out_dim))\n",
    "\n",
    "    y_hat = classifier_output(x,params)\n",
    "    print(\"y_hat: {}\".format(y_hat))\n",
    "    loss = logloss(y, y_hat)\n",
    "    y_diff = np.matrix(y_hat-y)\n",
    "    gW = np.dot(np.matrix(x).transpose(),y_diff)\n",
    "    gb = y_diff \n",
    "    if not np.all(gW.shape==W.shape):\n",
    "        print(\"problem with calculation of gW, size: {}, expected: {}\".format(gW.shape, W.shape))\n",
    "        print(\"shape y_diff: {}, shape x: {}\".format(y_diff.shape, np.matrix(x).shape))\n",
    "        raise AssertionError\n",
    "    return loss,[gW,gb]\n",
    "\n",
    "def logloss(y, y_hat):\n",
    "    print(\"in log loss. shape y: {}, shape y_hat: {}\".format(y.shape, y_hat.shape))\n",
    "    return np.dot(y,np.log(y_hat))\n",
    "\n",
    "def create_classifier(in_dim, out_dim):\n",
    "    \"\"\"\n",
    "    returns the parameters (W,b) for a log-linear classifier\n",
    "    with input dimension in_dim and output dimension out_dim.\n",
    "    \"\"\"\n",
    "    W = np.zeros((in_dim, out_dim))\n",
    "    b = np.zeros(out_dim)\n",
    "    return [W,b]\n",
    "\n",
    "def ll_sanity():\n",
    "#if __name__ == '__main__':\n",
    "    # Sanity checks for softmax. If these fail, your softmax is definitely wrong.\n",
    "    # If these pass, it may or may not be correct.\n",
    "    print(\"running softmax tests\")\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    test2 = softmax(np.array([1001,1002]))\n",
    "    test3 = softmax(np.array([-1001,-1002])) \n",
    "    print (\"test1: {}\".format(test1))\n",
    "    assert np.amax(np.fabs(test1 - np.array([0.26894142,  0.73105858]))) <= 1e-6\n",
    "    print (\"test2: {}\".format(test2))\n",
    "    assert np.amax(np.fabs(test2 - np.array( [0.26894142, 0.73105858]))) <= 1e-6\n",
    "    print (\"test3: {}\".format(test3))\n",
    "    assert np.amax(np.fabs(test3 - np.array([0.73105858, 0.26894142]))) <= 1e-6\n",
    "    print(\"softmax tests passed\")\n",
    "\n",
    "    # Sanity checks. If these fail, your gradient calculation is definitely wrong.\n",
    "    # If they pass, it is likely, but not certainly, correct.\n",
    "    # import sys\n",
    "    #sys.path.append(\"C:\\Shahar\\BarIlan\\NLP-courses\\89687-DL\\Assignment1\\code\\loglinear.py\")\n",
    "    #print(sys.path)\n",
    "    #from .grad_check import gradient_check\n",
    "    global W,b\n",
    "    W,b = create_classifier(3,6)\n",
    "\n",
    "    def _loss_and_W_grad(W):\n",
    "        global b\n",
    "        loss,grads = loss_and_gradients(np.array([1,2,3]),0,[W,b])\n",
    "        return loss,grads[0]\n",
    "\n",
    "    def _loss_and_b_grad(b):\n",
    "        global W\n",
    "        loss,grads = loss_and_gradients(np.array([1,2,3]),0,[W,b])\n",
    "        return loss,grads[1]\n",
    "\n",
    "    for _ in range(10):\n",
    "        W = np.random.randn(W.shape[0],W.shape[1])\n",
    "        b = np.random.randn(b.shape[0])\n",
    "        gradient_check(_loss_and_b_grad, b)\n",
    "        gradient_check(_loss_and_W_grad, W)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "f1: 15241.371590402501, f2: 15241.396281602501\n",
      "Gradient check passed!\n",
      "f1: 2.181276407852401, f2: 2.1815020283076203\n",
      "f1: 2.1814741082695495, f2: 2.181304327890471\n",
      "f1: 2.181432593192951, f2: 2.1813458429670707\n",
      "Gradient check passed!\n",
      "f1: 25.453235254823095, f2: 25.45284148622899\n",
      "f1: 25.453099750789395, f2: 25.45297699026269\n",
      "f1: 25.4530307233796, f2: 25.453046017672484\n",
      "f1: 25.45294919600082, f2: 25.453127545051267\n",
      "f1: 25.453049852584527, f2: 25.453026888467555\n",
      "f1: 25.452924291996556, f2: 25.453152449055526\n",
      "f1: 25.45297237321248, f2: 25.453104367839604\n",
      "f1: 25.453116360133567, f2: 25.45296038091852\n",
      "f1: 25.452955910693856, f2: 25.453120830358234\n",
      "f1: 25.452953854038594, f2: 25.453122887013496\n",
      "f1: 25.45320094759778, f2: 25.452875793454304\n",
      "f1: 25.453078591690357, f2: 25.45299814936173\n",
      "f1: 25.45291198425104, f2: 25.453164756801044\n",
      "f1: 25.4528399019266, f2: 25.453236839125484\n",
      "f1: 25.452817822796217, f2: 25.453258918255866\n",
      "f1: 25.453147025350543, f2: 25.45292971570154\n",
      "f1: 25.453044323751914, f2: 25.453032417300168\n",
      "f1: 25.45299602432282, f2: 25.453080716729264\n",
      "f1: 25.452898415854506, f2: 25.453178325197577\n",
      "f1: 25.453072822907664, f2: 25.45300391814442\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "STUDENT={'name': 'Shahar Siegman',\n",
    "         'ID': '011862141'}\n",
    "\n",
    "def gradient_check(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### modify x[ix] with h defined above to compute the numerical gradient.\n",
    "        ### if you change x, make sure to return it back to its original state for the next iteration.\n",
    "        v = x[ix] \n",
    "        x[ix] = v + h/2\n",
    "        f2,_ = f(x)\n",
    "        x[ix] = v - h/2\n",
    "        f1,_ = f(x)\n",
    "        x[ix] = v\n",
    "        print(\"f1: {}, f2: {}\".format(f1,f2))\n",
    "        numeric_gradient = (f2-f1)/h\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numeric_gradient - grad[ix]) / max(1, abs(numeric_gradient), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed.\")\n",
    "            print(\"First gradient error found at index %s\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numeric_gradient))\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next index\n",
    "\n",
    "    print(\"Gradient check passed!\")\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print(\"Running sanity checks...\")\n",
    "    gradient_check(quad, np.array(123.456))      # scalar test\n",
    "    x = np.random.randn(3,)\n",
    "    gradient_check(quad, x)    # 1-D test\n",
    "    gradient_check(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # If these fail, your code is definitely wrong.\n",
    "    sanity_check()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running softmax tests\n",
      "test1: [0.26894142 0.73105858]\n",
      "test2: [0.26894142 0.73105858]\n",
      "test3: [0.73105858 0.26894142]\n",
      "softmax tests passed\n",
      "y_hat: [8.35714812e-01 1.27598991e-01 4.01129364e-06 3.43675380e-02\n",
      " 1.46421570e-05 2.30000570e-03]\n",
      "y_hat: [8.35721677e-01 1.27593659e-01 4.01112602e-06 3.43661020e-02\n",
      " 1.46415452e-05 2.29990959e-03]\n",
      "y_hat: [8.35707947e-01 1.27604323e-01 4.01146125e-06 3.43689741e-02\n",
      " 1.46427689e-05 2.30010180e-03]\n",
      "f1: [0. 0. 0. 0. 0. 0.], f2: [0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-46415c3bd643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mll_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-286cde502e5c>\u001b[0m in \u001b[0;36mll_sanity\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[0mgradient_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loss_and_b_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mgradient_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loss_and_W_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-e625bc600f7f>\u001b[0m in \u001b[0;36mgradient_check\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnumeric_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Compare gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mreldiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric_gradient\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreldiff\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1e-5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gradient check failed.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "ll_sanity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18983994 -1.40727523 -0.5841523  -0.27341281  0.49363692 -0.99399498]\n",
      " [ 0.40646452  2.27836731 -0.47029934 -0.5459866  -0.67480659  0.53160206]\n",
      " [ 0.99333181  0.27874525 -1.27685437  0.4979319  -1.54597973 -0.01091921]]\n",
      "[ 0.21762629 -1.66469938 -2.69122412  0.88080604 -1.25783171 -1.73143664]\n"
     ]
    }
   ],
   "source": [
    "global W,b\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_and_W_grad(W):\n",
    "    global b\n",
    "    loss,grads = loss_and_gradients(np.array([1,2,3]),0,[W,b])\n",
    "    return loss,grads[0]\n",
    "\n",
    "def _loss_and_b_grad(b):\n",
    "    global W\n",
    "    loss,grads = loss_and_gradients(np.array([1,2,3]),0,[W,b])\n",
    "    return loss,grads[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat: [8.35714812e-01 1.27598991e-01 4.01129364e-06 3.43675380e-02\n",
      " 1.46421570e-05 2.30000570e-03]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-2c6dab490de8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_loss_and_W_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-b89b135af094>\u001b[0m in \u001b[0;36m_loss_and_W_grad\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_loss_and_W_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-7681b6d4cee9>\u001b[0m in \u001b[0;36mloss_and_gradients\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y_hat: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0my_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mgW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-7681b6d4cee9>\u001b[0m in \u001b[0;36mlogloss\u001b[1;34m(y, y_hat)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"in log loss. shape y: {}, shape y_hat: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "l, g = _loss_and_W_grad(W)\n",
    "print(l.shape)\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1001, 1002]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bari_env",
   "language": "python",
   "name": "bari_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
