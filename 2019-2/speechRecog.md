https://www.cs.biu.ac.il/~jkeshet

<div dir='rtl'>

# זיהוי דיבור

2019-03-06

יוסי עדי
יוסי ק יעביר את ההרצאות הראשונות השאר יוסי ע

עיבוד אותות

אלגוריתמים קלאסיים זיהוי דיבור

קשה? מצריך עבודה 

ארבעה תרגילי תכנות


היום: intro

דאטה של ספיצ' 1000 שעות מינימום לאימון (עולה עם הזמן)
גוגל, אמאזון - 50-60 *אלף* שעות.

מחשב שמנהל דיון debate.
אתגר שIBM המציאו.

רוב המחקר הוא על אנגלית. אין משהו מיוחד לשפה. יש גם על מנדרינית. נשמע כמו דיבור גם אם ההברות לא מרכיבות מילים.

התשמשו באותו מודל כדי להפריד דיבור לרעש רקע. ממש נשארות שתי קומפוננטות נפרדות של רעש הרקע והדיבור.

התחילו מהרבה אנליזה סטטיסטית, צמצום למספר מאוד קטן של מילים (כן/לא) וכד'.
expert systems. perception.

להגדיר לפי תדרים איזה הברה אמרתי.

היום: להתעלם מידע קודם ולהסתמך על הרבה דאטה.

בשנות ה70 80 התפתחו האלגוריתמים המודרנים אבל חיכו עוד עד שהחומרה תאפשר שימוש בהם.

### למה זו בעיה קשה?
תמונה: גודל קבוע (resize אם צריך) 
בדיבור: גם הקלט וגם הפלט מורכבים. אין גודל קבוע. אפילו אם אני אומר את אותה מילה זה בכלל לא נראה דומה.

במילישניה אין אינפורמציה (בניגוד לוידאו שכל פריים מכיל הרבה אינפורמציה)

הרבה data שמתאמנים עליו 
זה אנשים שקראו ספר. 
אבל ביום יום מדברים אחרת.
כדי לאמן מודל לאימון הספרות שאני מקריא בשיחה עם הבנק אולי הקראת ספר בכלל לא עוזרת.

אין כמעט דאטה סט spontaneous/ continuous. כמו שאנשים מדברים עם "אה..." וכיו"ב

המקרופון גם חשוב, האקוסטיקה משתנה ומצריכה כיול מחדש. אפילו עם אותו טלפון אם אני מפנה את הראש הצידה זה כבר נשמע אחרת והמערכת לא מצליחה להבין.

אתר WER ARE WE מראה את הבנצ'מרקים בתחום. ירידה מ40% לפני 22 שנה ל? היום.

הALEXA של מיקרוסופט עם 3 מקרופונים.

### איך נראה סיגנל דיבור
הזמן בציר X, ובציר Y אמפליטודה. 
אני צריך לדגום אותו.
דיבור דוגמים 16000 הרץ. דיבור 44100.

$A \in \mathbb{R}^{16000}$


[הסבר על למה מרחק אוקלידי לא עובד וצריך ייצוג יותר סמנטי של הסיגנל]

עושים FFT ואז DYNAMIC TIME WARPING

תוכנה שנקראת PRAAT.
 מראה FFT על כל מקטע זמן קצר.
 

Overtone - התופעה של ריבוי תדרים שיוצאים מחלל תהודה אמיתי (גוף האדם)

יש אלגוריתמים לזיהו התדר הבסיסי

### Data driven vs. knowledge driven
שביל הזהב בין ללמוד ממומחים ללהתעלם ממומחים. למשל פונמות זה חשוב. יש משפחות של עיצורים.
* voicing - עם ויברציה של מיתרי הקול
* fricative - ש,ס
* בגד כפת - פוצצים

פונמות פחות תלויות שפה. יש באנגלית 40-50.

[הזכיר את התעתיק הפונטי המופיע במילון]

[הציג גרף רגישות לתדרים של אוזן אנושית]

זרק מושג mel frequency/ mel scale.
היה הקונבנציה, DL מצליח ללמוד בלי הנרמול הזה.
בפועל הראו שלומד משהו דומה (אבל לפעמים יותר מוצלח)

משבוע הבא יותר מתמטי פוריה וDL כמה שעורים



---
2019-03-13

נתחיל מעיבוד אותות

סיגנל אנלוגי $X_a(t)z$ . גלי הקול מרטיטים מגנט שנע בתוך סליל ומייצר וריאציות במתח.

רמקול זה אותו דבר רק בכיוון הפוך. צריך מגבר כדי להניע את הרמקול מספיק חזק.

סיגנל אנלוגי לא מתאים למחשב, צריך להמיר אותו לסידרה. $x[n]z$. דיסקרטיזציה.

**דיסקרטיזציה**
</div>

$x[n] = x_a(nT)$ continuous to discrete: C/D
<div dir='rtl'>

איך נקבע הT? 
בעזרת **Nyquist-Shannon Theorem**:
אם נדגום בקצב מהיר שהוא פי שניים מהתדר הכי גבוה $f_{max}z$ 
נוכל לשחזר את הסיגנל בצורה מושלמת

ההנחה בהשמה $X[n]z = X_a(nT)z$ היא שהדיוק של ייצוג האמפליטודה הוא אינסופי.
במקום C/D שהוא קונספטואלי
משתמשים ב A/D. נותן דיוק סופי. 
לזה אין משפט נייקוויסט.

</div>

#### 1. delta function

$$\delta[n] = \left\{ 1,\ n=0; 0,\ n \ne 0 \right\}$$

#### 2. step function

$$u[n] = \left\{ 1,\ n \ge 0; 0,\ n \lt 0 \right\}$$

##### 2.1 relation of $u$ and $\delta$
$u[n] = \sum\limits^{\infty}_{k=0}\delta[n-k]$
$\delta[n] = u[n] - u[n-1]$
#### 3. exponential function
$$x[n] = A\alpha^n,\ A,\alpha \in \mathbb{R}$$

#### 4. Sinusoidal function

$$x[n] = Acos(\omega\cdot n + \phi)$$

#### 5. Complex exponential function

$$A=a+ib = |A|e^{i\phi}$$
$$|A| = \sqrt{a^2+b^2},\ \phi = \tan^{-1}\frac{b}{a}$$
$$X[n] = Ae^{i\omega n} = |A|e^{i\phi}\cdot e^{i\omega n} = |A|e^{i(a\omega n + \phi)}$$



<div dir='rtl'>

סיגנל מחזורי בטבע אינו בהכרח מחזורי במחשב. התנאי שהדגימה תחזור על עצמה בדיוק, הוא שהתדר הוא כפולה שלמה של תדר הדגימה.



#### מערכות ומערכות LTI

</div>

$y[n] = H(x[n])$

Linear: $H(\alpha_1 x_1[n]+ \alpha_2x_2[n]) = \alpha_1H(x_1[n]) + \alpha_2H(x_2[n])$
Time invariant: $y[n-n_0] = H(x[n-n_0])$

$h[n] \equiv H(\delta[n])$

<div dir='rtl'>
מכאן הדימיון (הכמעט מוחלט) בין "סיגנל" למערכת LTI

</div>

$$y[n] = \sum\limits_{k=-\infty}^{\infty} h[n-k]\cdot x[k] \equiv h[n]*x[n]$$
where $*$ signifies _convolution_. הפעלה" של מערכת".


#### תנאי יציבות
$\sum\limits_{n=-\infty}^{\infty} |h[n]| \lt \infty$


##### דוגמא
$h[n] = A\alpha^n\cdot u[n]$

$\sum h[n] = \sum A\alpha^n u[n] = \sum A\alpha^n = \frac{A}{1-|\alpha|} \lt \infty$

### אנליזת פוריה לסדרות בדידות
DTFT:
$X(\omega) = \sum x[n] e^{-i\omega n}$

$X(\omega)$:
* Complex
* periodic with period $2\pi$
* $x[n] \in \mathbb{R} \Rightarrow X(\omega) = X(-\omega)$ (Symmetric) 

Inverse transform:
$x[n] = \frac{1}{2\pi} = \int\limits_{-\pi}^{\pi}X(\omega)e^{i\omega n}$

#### דוגמאות לשימוש בFourier

1. $x[n] = \delta[n-n_0] \Rightarrow X(\omega) = \sum \delta[n-n_0]e^{-i\omega n}  = 1\cdot e^{-i\omega n_0}$

1. $x[n] = a^nu[n] = a^n,\ n\ge0 \Rightarrow X(\omega) = \sum a^n e^{-i\omega n} = \sum (a e^{-i\omega})^n = \frac{1}{1-ae^{-i\omega}}$


----

March 27 2019 YK


</div>
<div dir='rtl'>
אחד הבסיסים הלינאריים האפשרים הוא ייצוג של סינוסים. DTFT.
בייצוג הזה יש הרבה יותר הדירות בין דגימות של אותה מילה.

</div>

$$x[n]=\frac{1}{2\pi}\int\limits_{-\pi}^{\pi}X(\omega)e^{i\omega n} d\omega$$

$$X(\omega)=\sum\limits_{n=-\infty}^{\infty}x[n]e^{-i\omega n}$$

$e^{i\omega n} = \cos(\omega n) + i \sin(\omega n)$

<div dir='rtl'>

w תדר.נמדד ביחידות $rad/sec = 2\pi/sec$ 
</div>

$\omega = 2 \pi f \rightarrow f $ frequenency measured in $1/sec = Hz$


<div dir='rtl'>
אפשר ייצג את x[n]
ע"י אינסוף סינוסים, כ"א בתדר w
אחר.
</div>

x[n]: time domain
x(w): frequency domain


<div dir='rtl'>

#### X(w)z קומפלקסי
מה עושים עם הקומפלקסי? רוב האינפורמציה נמצאת במגניטודה, אבל כדי לשחזר צריך את הפאזה. כדי לנתח לוקחים את  $|X(\omega)z|$.

#### עבור x[n]z ממשי: X(w)=x(-w)z


#### X(w)z מחזורי עם מחזוריות 2&pi;

מה שזה אומר שכל הסיגנל נמצא בין 0 ל &pi; (אפשר לשחזר את הסיגנל מהערכים בטווח הזה)

מה זה פאזה? זה הדיליי של כל סינוס. כל אחד חוזר בפאזה שונה. בהידהוד בחדר יש חשיבות לפאזות כי ההד מעוות את הפאזה של הדיבור המקורי.


</div>

$x[n] = A\cos(\omega_0n + \phi) \Rightarrow$

$X(\omega) = \sum\limits_{n=-\infty}^{\infty} A\cos (\omega_0n+\phi)e^{-i\omega n} \\
= \frac{A}{2}\sum_n[e^{i(\omega n + \phi)} + e^{-i(\omega n + \phi)}]e^{-\omega n i}\\
\ \\
= \frac{A}{2} e^{-i\phi} [\delta(\omega-\omega_0) + \delta(\omega+\omega_0)]\ [-\pi,\pi]$


<div dir="rtl">

כל vowel
מורכב משני תדרים עיקריים. יש מפות. בצרפתית יש 24 סוגים שונים של vowels.


#### מערכת הדיבור
vocal cords - the note. the frequency.
vocal tract - the cavities of the mouth and noise - _what_ we say

מיתרי הקול מייצרים איזשהו סיגנל בסיסי. לא, לא סינוס, אלא סידרה אינסופית של פולסים במרחק P ביניהם.
Pulse train.
הגוף מתאמץ לסגור, בגלל הזרימה של האוויר מהריאות הם נפתחים בפולס ונסגרים מיד בחזרה (גם בגלל ברנולי).
אמרנו שבמישור התדר הסיגנל רציף. 


</div>

---

3 April 2019 YK
####חזרה

sampling
$x[n]=X_a(nT)$

Fourier
$x[n]=\frac{1}{2\pi}\int\limits_\pi^\pi X(\omega)e^{i\omega n} dw$ DTFT

$X(\omega)$ = $\sum\limits_{n=-\infty}^{\infty}$ $x[n]e^{-iwn}$ .



impulse train with spacing P &harr; impulse train with spacing $\frac{2\pi}{p}$ .

<div dir="rtl">

הבעיה עם דגימה היא כאשר תדר טהור שקיים בסיגנל המקורי עלול ליפול בין שני תדרים בדגימה. התוצאה היא קונבולוציה עם פונקציית סינק, כלומר מריחה של התדרים סביב התדר האמיתי. כשיש שני תדרים מקוריים קרובים, לא נוכל להפריד אותם.

### משפט הדגימה

אם $X_a(t)z$ 
דגום כל T 
שניות 
$F_s=1/T$
וגם התדר הגבוה ביותר של 
$X_a(t)z$ 
הוא 
$F_{max}$
אז אפשר לדגום את 
$X_a(t)z$ 
בתדר גבוה מ 
$2F_{max}$
ולשחזר את 
$X_a(t)z$
בצורה מושלמת


מה קורה אם התדר הגבוה ביותר גדול מתדר הדגימה?
![aliasing](SpeechRecog/aliasing.png)

<div dir='rtl'>

#### מודל הפקה של דיבור
![speech system](SpeechRecog/speech_physiology.png)

#### שיערוך מעטפת הספקטרום
התחילו מדחיסה. האם אפשר לבטא את האבר ה n כסכום של אברים שלפניו כפול מקדמים. 
נעשה אופטימיזציה כלומר נמצא שגיאה ריבועית מינימלית של המקדמים.


<div dir='ltr'>


$s[n] \approx \sum\limits_{i=1}^p a_is[n-i]$

$e[n]=s[n] - \sum\limits_{i=1}^p a_is[n-i]$

$\min\sum e^2[n] = \sum\left(s[n]-\sum a_is[n-1\right)^2$

$\frac{\partial \sum_n e^2[n]}{\partial a_k}=0$

$sum_{n=0}^m s(s[n]-sum_i a_i sn-i(-s[n-k])=0$

$$\sum_n s[n]s[n-k]=\sum \sum$$

$$\phi(m,i)=\sum_{n=0}^m s[n] s[n-i]$$ 
autocorrelation

הדרך למצוא את התדר היסודי של הדיבור זה להסתכל על אוטוקורלציה. הפסגה המשמעותית הראשונה היא במרחק של זמן מחזור של התדר היסודי.




"Linear predictive coding"

----

<div dir='rtl'>

10 Apr 2019
YA

Speech Recognition and Processing - lecture 6
בואו ניקח את KNN, ונסווג דוגמה חדשה לפי הכי דומה
יש לי שתי בעיות - איך לייצג ואיך להתאים alignment.
זה יהיה התרגיל.

DFT או FFT הסתכלנו על תדרים בכל הסיגנל.

dft.ipynb


תנועות יש להם מספר מובחן של תדרים. עיצורים פוצצים למשל פ,ב, מרוחים על כל הספקטרום.
חיכוחיים (ש) הרבה תדרים מרוחים.

אם אני מחפש n תדרים לכאורה זה n&sup2;. יש אלגוריתם FFT שמשפר.


### Dynamic Time Warping

פעם הגישה היתה להבדיל בין מספר מצומצם של מילים, לזהות את המילה כיחידה. זה סוג של 1NN מרחק בין הדגימות של המילים האפשריות למילה הנוכחית שאני מנסה לסווג.

DTW מותח ומשכפל פריימים.

אני אזוז ימינה, למעלה או באלכסון באופן שמוצא את המלסלול המתאים ביותר. בכל משבצת יש את כל התדרים בפריים.

זה מתאים לזיהוי מילים בודדות, פחות מתאים למשפטים מלאים.

נפתור בעזרת dynamic programming. 

שאלה: מה אורך בזמן של רצועה? איך אני קובע? לרוב בזיהוי דיבור זה באזור 20 מילי. לזיהוי דובר מספיק 50 מילי. למטרות מסויימות שצריך לזהות משהו מאוד מהיר, 1 מילי.

יש פעולה שנקראת MFCC ונכווץ אותו. צ'אנקים של תדרים. זה עוזר למנוע מרחק אוקלידי שמאוד רגיש להבדלים קטנים בתדר.

---
<div dir='rtl'>

1 May 2019 Lecture 7

## ASR - automatic speech recognition. 
phoneme, acoustic, language.

עד עכשיו ראינו לקחת את המרחק המינימלי לפי מטריקה כלשהי. על כל המילון. מילים שלמות, אין למידה, זה לא הסתברותי, קשה להגדיל את המילון.
.

חסרונות של סיווג מילים שלמות:
- מילה זה ארוך, עלולים לחצות אותה בפריים בטעות
- שפה משתנה לאורך הזמן
- איך לשבור את המילה - לא לאותיות.
- מבטאים

### Phonemes
פונטיקה - לימוד התכונות של קולות הדיבור.
Articulatory, Auditory, Perpective

פון - בניגוד לפונמה - כל צליל הניתן להפקה.
פונמה - משמעות סמנטית.

#### Kit vs. Skill

-רואים שיותר ארוך ב kit
- ההטעמה משנה
- אפילו באמתע מילה יש עצירות של 50 מילישניות כדי להתאים את הפה להברה הבאה.
- הברה היא פחות או יותר פונמה.

CMU dictionary - מאפשר לעבור בין כתיב "רגיל" של מילה לתעתיק של רצף פונמות. באנגלית יש 65 פונמות אבל דחסו ל39.
 לפעמים אנחנו נדרשים לתהליך ההפוך - פענחתי פונמות ואני כותב איזה מילה זו בלי להכיר אותה. צריך "לאזן" בין ההסתברות שנאמר משהו אחר בדרך סטנדרטית לזה שנאמר משהו סטנדרטי בדרך אחרת.

</div>

<div dir='ltr'>

$w^* = \arg \max\limits_wP(w|o) = \arg \max P(o|w) \cdot P(p|w)$

ASR structure

$w^* = \arg \max\limits_w \max \limits_p P(o|p) \cdot P(p|w) \cdot P(w)$

language model - לפעמים פוגע בביצועי הזיהוי האקוסטי אם הוא חזק מדי.

גם הפונמות הם לא אטומים, יש לנו פריימים, פונמה אורכה 1-20 פריימים, לכן בפועל הניתוח הוא של 
P(o|q)
כאשר q 
הוא הפונמה של הפריים.

בעיית ה P(o|q)
GMM - gaussian mixture model

---

May 22 2019
lecture 8
<div dir='rtl'>

יש שלוש קומפוננטות. ניגע ב E2E. CTC connectionist temporal classification
מודלים ברמת התו (אות)

המגמה היום זה לעבוד ישירות עם האות. הקונבולוציה מוציאה פיצ'רים.
סטרטאפים עובדים DNN-HMM.

language model, pronunciation model, acoustic model. &rArr;  HMM.

לפעמים יש מודול שמיועד למצוא קרבה בין רצף פונמות "תקני" של מילה לרצף פונמות שהתקבל.

בין מילים יש פונמה של שקט.
מילים out of vocab כמו "ummm..." יכולות להיות בעיה.

לחפש על כל המילים זה מרחב ענק. רצפים מילים- עוד יותר גדול.

הדגיש שהתיוג של ה train set חייב לכלול את הalignment בין הפונמות לפריימים האקוסטיים. מצד שני נקודת המעבר מפונמה לפונמה לא מוגדרת ולהשיג תיוג מסוג זה זה מאוד יקר. יש רק שני דאטא סטס כאלה, לעומת הרבה שהם לא aligned.

דוגמה מ TIMIT שנראית כמו כתוביות: ממילישניה, עד מילישניה, פונמה.

עד 2006 אמרו כך: אני יכול לאמן על ש

נתון לי שנאמר cat. רוצה למצוא
P(cat | o)
הסתברויות המעבר יהיו נתונות בחלק מהמקרים. יש bauman-welsch שזה לעשות forward-backward על ה *---*

דיבר 9:35-9:45 על זה שצריך ללמוד אליינמנט בתהליך האימון, כי אני חייב לכאורה לאמן עם אליינמנט, זה מסובך כי צריך להריץ ויטרבי בדרך כדי להחליט מה האליינמנט הנכון. אולי נעשה אימון על קלט ללא אליינמנט ובלי שנחליט בכלל מה האליינמנט הנכון? נשתמש בתוחלת על הכל.

אימון E2E. כשאני מסיים את האימון אני יכול לבדוק את עצמי ע"י חישוב עם האליינמנט המקסימלי ורואים שיוצא מאוד דומה, כלומר בכל מקרה השתמשתי באופן חבוי באליינמנט המקסימילי

יש גם את עיניין הפונמות השקטות שנכנסות כרעש לאימון מיושר

בתחום speech recog יש הרבה בעיות הנדסיות שנפתרות ע"י אלגוריתמים.

### CTC - connectionist temporal classification
קודם כל מכניס סימן &epsilon; שמייצג שקט. זה לייבל חשוב כדי לעשות ייצוג יותר נכון של המעבר מאקוסטיקה דרך פונמנות למילים.

3. הסיגנל תמיד ארוך מה transcription 



---

May 29 2019 Y.A.


### CTC המשך/חזרה

הסביר שוב את העיקרון של ה CTC training:
1. הרשת מוציאה הסתברות לפונמה עבור פריים

ממקסם את כל המסלולים, לא אכפת לי באיזה דרך אני ממקסם את המסלולים.

CTC הוא אדפטיבי יחסית במקרה שהנתונים בטסט נובעים מ"שימוש אחר בשפה". בשונה מHMM שבו השפה נכנסת כהסתברות בצורה חזקה.

בפועל רוב המשקל יגיע מה MAX. ככה הCTC עובד. רוב הזמן הוא נותן בלנק. הוא לא מקבל "עונש" על זה וזה "אסטרטגיה טובה" למיזעור לוס.

לאמן CTC טוב על 4 GPU זה יקח חודש. 1000 שעות. יש כזה דאטהבייס באקדמיה. בתעשיה כמובן הכל בסקאלות הרבה יותר גדולות.

מ2015 עושים CTC מול תווים במקום מול פונמות, הDNN נהיו מספיק חזקים בשביל זה.

#### הסקה 
איך אני מחליט מה המילה הראשונה שלי? מרחב החיפוש ענק.
התשובה: beam search. מתחזק את K האפשרויות הסבירות ביותר.
הסתברויות המעבר אפשר למשקל פנימה מודל שפה. יש מודל של אותיות, של מילים, ...

יש התאמה של beam search לCTC כי במהלך ההסקה יש כמה מסלולים שיהיה להם אותו decoding (עקב התעלמות מרצפים זהים). 

Deep speech - מודל שפותח בBaidu. יש מימושים פתוחים ויש גם רשתות מאומנות פומביות.

#### מדידת שגיאה
סוג של edit distance: word error rate / character error rate

Augmentation: 
*  להוסיף רעש בכוונה (של מסעדה, של כביש).
* מתיחה וכיווץ
* שינויי pitch
* reverb (אקוסטיקה בכנסיה)

אני עושה וריאציה בדברים שבהם אני רוצה שהרשת תדע לפענח בצורה יותר רובוסטית.

הוספת של שכבות משפרת משמעותית. הוספת של BN גם משפרת.

כשיש acoustic model ו language model שאומנו בנפרד, צריך לוודא שאני מאזן ביניהם.

Wav2Letter - אלגוריתם של FB.
* עצבן אותם ה &epsilon;
* לכתוב את המספר של התוים החוזרים במקום לחזור עליהם
* מפשט את את הגרף המקופל של המעבירים


----

2019-06-05 Y. Adi


למה לא רק HMM?
המעבר מ $o_t$ ל $q_t$ הוא קשה. צריך משהו כמו DNN, או פעם SVM. 


בניגוד להתקדמות עם תמונה וגם NLP, ששם יש הרבה בעיות שנפתרו, ושהעלו פתרונות ל PUBLIC DOMAIN, בדיבור אין את זה. בתמונה לא מפתחים יותר פיצ'רים, הכל ה DNN יכול לעשות יותר טוב בעצמו.

ה listen attend spell הוא הכי קרוב לקליע כסף.

מודל RNN החליף בNLP את ה n-grams. כי יכולים להיות תלויים בטווח הארוך.

כשהתחילו לעשות תרגום אחת הבעיות היא שמודלי סדרה צריכים מספרי אאוטפוט קטן שווה מספר אינפוט. ואז עשו אנקודר דיקודר וזה עובד יפה עד 20 מילים. מעבר לזה מתחיל לזייף, הביאו h ל גודל 1000 והמודל כבר מאוד גדול.

ההמשך הוא alignment: attention mechanism.

alignment יכול לעזור גם בזיהוי דיבור. זה מה שגוגל טוענים שהוא מנגנון זיהוי הדיבור המוביל שלהם.

האימון הוא מול רצף של אותיות. זה דאטה סטנדרטי בלי בעיית הalignment שיש במודלים פשוטים של דיבור.

הם ניתקלו בבעיה שהאורך של הקלט גדול משמעותית מהפלט. אורך ה LSTM היה יוצא מאוד גדול. הפתרון הוא צמצום אורך הקלט במודל פירמידה. אחת המטרות הוא שלא צריך בנוסף language model.

sampling - למשקל בין הזנת $\hat{y}$ ל $y$.

הattention עבד ללא אילוצים, למשל את עיניין הסדרתיות הוא למד בעצמו.

### טרנדים נוכחיים בלמידת דיבור
* DNN-HMM
* CTC
* WAV2LETTER
* LAS
כולם מתבססים על פיצ'רים כמו mel וכד'
מה לגבי הכנסת הסיגנל המקורי על ציר הזמן? 
* כל נקודת דאטא בפני עצמה חסרת משמעות 
* המון קונטקסט

פתרון: קונבולוציה חד מימדית על הסיגנל מאותחלת לDFT סטנדרטי.
אפשר לשחק עם הstride וכד'.

