<div dir='rtl'>

### התרגיל1 
תרגיל: לממש רשת נוירונים בנמפיי בלבד. 

כל ספריה של למידת מכונה אסורה.

forward pass, training, back propagation.

מי שכותב בC, C++ יקבל בונוס.

מגישים פרדיקציות על טסט דאטה.

כל שורה מתחילה בלייבל. יש 10 קטגוריות. 3000 פיצ'רים. תחליטו כמה שכבות hidden.
פלט 10 נוירונים. 
להגיש: קוד, הוראות, הרצה.

תקבעו את ה seed ככה שתמיד יצא אותו דבר.

###
overfitting היא עליית ציון ב trainining שלא מותאמת ב test.

מה אפשר לעשות כדי "להפריע" לרשת ללמוד "בע"פ". פעם אמרו תוריד את מספר הפרמטרים, היום מבינים שזה לא נכון, אתה עלול לפגוע ביכולת הכללה.

איך מפריעים? במישור הקלט או במישור הלמידה Data augmentation

#### Data augmentation methods
פעם חשבו שאם הדאטה ענק לא יהיה OF. זה לא נכון. 
* input noise בהסתברות מסויימת נוסיף רעש, למשל גאוסיאני לפיקסלים. היום במקום גאוסיאן מכבים למשל 20% מהפיקסלים. תעשו את זה זה עוזר במיידי.
יש sweetspot איפשהו בין 0% רעש ל 100% רעש.
בכל epoch פיקסלים אחרים יוצאים.

דיון בכיתה - למה דווקא לאפס את הפיקסל? אולי עדיף להחליף לממוצע? 
* הזזות והיפוכים- לשים לב כי זה domain specific. בדומה jittering, elastic.

#### avoiding overfitting during training
הפרעה אחרי למידה היא לא אפקטיבית כי אתה לא יודע איפה להתערב.
* weight decay - איתרתי את המאמר המקורי משנות השמונים. השראה מהמוח האנושי. 
כוח הרסני על כל הסינפסות. כשאנחנו ישנים הסינפסות דועכות. משתמשים בנורמה הראשונה או השנייה. 
היה לנו
</div>

$w'=w+\alpha\cdot x \cdot \Delta$

עכשיו נוסיף פרמטר

$w'=w+\alpha\cdot x \cdot \Delta- d\cdot w,\ d \approx 1/10,000$

<div dir='rtl'>

עד האפוק הבא,שבו הם יראו את החתול שוב. אם הסינפסה חשובה, התוספת תהיה בטלה. אם באמת לא משתמשים בזה, עד אותו חתול שוב זה יעלם.
&alpha; יורד עם הזמן לכן חשוב להכניס את הd:

</div>

$w'=w+\alpha\cdot (x \cdot \Delta- d\cdot w)$


<div dir='rtl'>

זה שהW מופיע פה לינארית, זה אומר שעשינו עונש פרופורציוני לנגזרת שניה.
אם רוצים להעניש את הנגזרת הראשונה

</div>

$w'=w+\alpha\cdot (x \cdot \Delta- \min(d,abs(w))\cdot sign(w) \cdot w)$


<div dir='rtl'>
כשהתחילו על ReLu ישר שמו רגולריזציה.
למה עדיף L2? כי L1 מעניש את כולם באותה מידה. L2 יותר הגיוני. אתה לא רוצה להפריע למשקולות קטנים, הם מייצגים מידע.

### DropOut
עבור כל בעיה, נאמן רשת מס. 1, נאמן שוב נקבל רשת מס' 2 . ככה כמה פעמים. אם נעשה ממוצע של כולם נקבל דיוק יותר טוב. אבל לא עושים את זה כי זה יוצר עומס חישוב. איך אפשר להרוויח את אותו אפקט בלי לשלם חישובית.? 
עבור כל דוגמה שמכניסים לרשת, באופן רנדומלי מכבים 50% מהנוירונים בשכבות הhidden. 

איך תממשו? תבנית של 0 ו1 אקראית ותכפילו את האקטיבציה. יש כאילו דגימה מהרבה רשתות שונות. במאמר כתוב prevents co-adaptation of neurons. 
כמו סטודנט שמציץ מעבר לכתב של הסטודנט הטוב. 
הופך את הנוירונים לעצמאיים יותר. 

רגע, בטסט הנוירונים יהיו חיים. מה הבעיה עם זה? יש הרבה יותר נוירונים שיורים לתוכו. מה עושים? אם הdropout rate היה do, אנחנו נכפיל ב$(do-1)$ יש כמה שיטות: סקאלה לפלט, סקאלה לפלט, סקאלה למשקולות. כל השיטות שקולות

### DropConnect
אותו רעיון רק על weights. הרבה יותר כבד חישובית והתוצאה פחות טובה.

----

April 7th

יש לנו הבסיס רשתות fully connected מספר שיטות למניעת overfitting.

עכשיו נלמד רשת convolutional. כל היישומים שקשורים לvision מבוסס על CNNs.

עוד לפני כן, נסתכל על רשת בסיסית. 

בכל רשת יש יתירות. כשמשתמשים בכל מיני שיטות להעיף משקולות אתה מצליח בלי שהדיוק יורד. אנחנו לא יודעים לעשות את זה מראש.

המשקולות מאוחסנים במטריצה ולכן לאפס חלק מהמשקולות לא מפשט את החישוב. לחילופין sparse matrix.
מאידך גיסא כל התשתיות מבוססות על טנסורים. לכן בלי קוד יעודי אי אפשר להאיץ בכלל, עם קוד יעודי אפשר להאיץ/ לדחוס קצת, אבל אף אחד לא עושה את זה. geoffery hinton אומר שיש קצ', החוקרים לא נכנסים למימוש כי קשה להתחרות בביצועים.

אפילו CNN שהוא בעיקרון sparse ממומש כ dense(?).


### Convolution
פעולה מתחום עיבוד תמונה. דמיינו שרוצים edge detection. יש לנו פילטר למשל 3x3.
לרוב הגודל אי זוגי כדי שיהיה ברור מי במרכז.
אחרי הפעלה על פיקסל, נותנים ערך חדש באותה נקודה.
לוקחים את הפילטר, מלבישים את הפיקסל שמוגדר כמרכז על הפיקסל שרוצים, ועושים dot product בין הערכים של הפילטר לערכים התואמים בתמונה.

עכשיו נזיז אותו אחד ימינה (קצב ההזזה - stride). אם הוא גדול מ-1, התמונה בפלט תהיה קטנה פי 2.

איך מטפלים באפקט הקצה? או שהתמונה בפלט קטנה יותר ב-2 בשני המימדים, או (הרבה יותר נפוץ) zero padding. מבטיח את שמירת גודל התמונה.

מה לגבי תמונה צבעונית? הקלט תלת מימדי, בהתאם הפילטר תלת מימדי. הפלט הוא תמיד דו מימדי. סכום של הסכומים. התמונה בפלט נקראת feature map. 

דוגמה. נכון אם יש רעש גאוסיאני נעשה ממוצע של פיקסלים סמוכים? אתם יכולים לתאר מסכת קונבולוציה שעושה אותו דבר? 1/9, 1/9 וכו' פותר

רעש salt and pepper השחירו והלבינו חלק מהפיקסלים. מה הפתרון? median. מבטל ערכים קיצוניים. אבל זה פעולה לא לינארית, לא נתנת לתאור בקונבולוציה.

בואו נחשוב על פילטר של זיהוי שפה. שפה היא שינוי צבע חד. Sobel filters

</div>

a | b | c
--|---|--
-1| 0 | 1
-1| 0 | 1
-1| 0 | 1

a | b | c
--|---|--
1| 1 | 1
0 | 0 | 0
-1| -1 | -1

זוהי נגזרת ראשונה (בשני הכיוונים) עכשיו נראה לפלסיאן:

a | b | c
--|---|--
0 | 1 | 0
1| -4 | 1
0 | 1 | 0

a | b | c
--|---|--
1 | 1 | 1
1| -8 | 1
1 | 1 | 1


<div dir='rtl'>

הפלט הוא מטריצת ערכים. זה לא תמונה, אם כי אפשר לנסות להציג אותה למשל לקחת ערכים מוחלטים.

כדי לקבוע משקולות משתמשים בידע בעיבוד תמונה. yan LeCunn אמר ב1989 שאפשר ללמוד את הweights.
איך זה עובד?

האינפוט מוגדר, נגדיר פונקצית מחיר, מה נשאר? זה NN. יש fully connected מהסביבה של כל נוירון לנוירון בשכבה הבאה. תהיה גם פונקציית אקטיבציה כמובן. כדי שזו תהיה קונבולוציה, צריך שכל מסכה תהיה עם אותן משקולות. יש רק 9N משקולות, אבל רק 9 שונות.
לא נגריל שונים לכל מיקום, נעתיק ממיקום למיקום. 

בתוך channel, אותם הערכים. בין channels, ערכים שונים.

אני יכול להפעיל על אותה תמונה כמה קונבולוציות. יכול לעשות כמה שבא לי. נגיד 10. כל ה10 ביחד זה שכבה בודדת של CNN. מורכבת מ M input channels, N output channels, כל פילטר בגודל. 
סה"כ מספר הweights בשכבה.
גודל המסכה יכול להיות שונה לכל אלמנט בשכבה? כן. א-ב-ל זה שובר את האחידות במימוש. אי אפשר לייצג כטנזור ארבע מימדים NxMxWxH.
כדי לסמלץ גדלים שונים משמתמשים בW, H המקסימלים הנדרשים ומאפסים ערכים בשכבות הקטנות יותר. וצריך לאפס אחרי כל מעבר backpropagation.

### backpropagation in CNN
זה פשוט אבל המאמרים לפעמים מבלבלים. weight update תמיד קורה ברמת הweight. סוכמים את כל השגיאות שקשורות לweight.

$w' = w+ \alpha \lambda \Delta$
* &alpha; - learning rate
* &lambda; - input
* &Delta; - error coming in from output neuron.

